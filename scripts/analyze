#! /usr/bin/env python

# Python 3 forward compatability imports.
from __future__ import print_function
from __future__ import division
from __future__ import absolute_import
from __future__ import unicode_literals

# Standard imports
import sys
import os
import io
import re
import argparse
import uweclang
from uweclang import is_punctuation
from itertools import chain
from collections import OrderedDict, Counter

from pprint import pprint

# We want to write all files in UTF-8 encoding. (This is a Python 3 forward
# compatability issue.)
reload(sys)
sys.setdefaultencoding('utf8')

# Script global variable for setting the extension for output files.
_OUTPUT_EXTENSION = ''

def analyze_text(text, file_name=None):
    """Takes a tagged text string and ouputs a dictionary containing the
    analysis of the text.

    Arguments:
        text (str): The input tagged text.
        file_name (Optional[str]): The name of the text file. Defaults to None.

    Returns:
        (dict): The analysis of the text.
    """
    data = OrderedDict([('File', os.path.basename(file_name))])

    tagged = uweclang.read_tagged_string(text)
    tokens = list(chain.from_iterable(tagged))

    # Count sentences and tokens.
    data['Sentences (Total)'] = len(tagged)
    data['Tokens (Total)'] = len(tokens)

    data['Tags'] = Counter()
    data['Words'] = Counter()
    data['Nouns'] = Counter()
    data['Nominalizations'] = Counter()
    data['Verbs'] = Counter()
    data['Modified Nouns'] = Counter()
    data['Additive Adverbs'] = Counter()
    data['Linking Adverbs'] = Counter()
    data['Passives (Total)'] = 0

    for t1, t2, t3 in uweclang.ngram(tokens, 3):

        # Lowercase non-proper noun words.
        t1, t2, t3 = tuple((t[0].lower() if t[1] not in ['NNP','NNPS'] else t[0], t[1])
                            for t in [t1, t2, t3])
        word, tag = t1

        # Count the number of occurences of each tag and word.
        if not is_punctuation(word):
            data['Words'][word] += 1
        data['Tags'][tag] += 1

        # Count nominalizations
        for suffix in uweclang.NOMINALIZATION_SUFFIXES:
            if word.isalpha() and word.lower().endswith(suffix):
                data['Nominalizations'][suffix] += 1

        # Count verbs
        if tag in ['MD', 'VBD', 'VBP', 'VBZ']:
            data['Verbs'][word] += 1

        # Count passives
        if word in uweclang.BE_FORMS and t2[1]=='VBN' or t3[1]=='VBN':
            data['Passives (Total)'] += 1

        # Count nouns
        if tag != None and tag.startswith('NN'):
            data['Nouns'][word] += 1

        # Count modified nouns
        if (t2[1] is not None and t2[1].startswith('NN') and
            tag != None and tag.startswith('JJ')):

            data['Modified Nouns'][(word, t2[0])] += 1

        # Count linking adverbs
        if (t1[0], t2[0], t3[0]) in uweclang.LINKING_TRIGRAMS:
            data['Linking Adverbs'][(t1[0], t2[0], t3[0])] += 1
        elif (t1[0], t2[0]) in uweclang.LINKING_BIGRAMS:
            data['Linking Adverbs'][(t1[0], t2[0])] += 1
        elif (t1[0], t2[0]) in uweclang.ADDITIVE_BIGRAMS:
            data['Additive Adverbs'][(t1[0], t2[0])] += 1
        elif t1[0] in uweclang.LINKING_ADVERBS:
            data['Linking Adverbs'][t1[0]] += 1
        elif t1[0] in uweclang.ADDITIVE_ADVERBS:
            data['Additive Adverbs'][t1[0]] += 1

    # Count words, puntuation, and ratios.
    data['Words (Total)'] = sum(data['Words'].values())
    data['Punc. Tokens (Total)'] = data['Tokens (Total)'] - data['Words (Total)']

    data['Words/Sentence'] = (data['Words (Total)'] / data['Sentences (Total)']
                              if data['Sentences (Total)'] != 0 else 0)

    data['Tokens/Sentence'] = (data['Tokens (Total)'] / data['Sentences (Total)']
                               if data['Sentences (Total)'] != 0 else 0)

    data['Tokens/Word'] = (data['Tokens (Total)'] / data['Words (Total)']
                           if data['Words (Total)'] != 0 else 0)

    # Calculate type/token ratio
    data['Type/Token Ratio'] = (len(data['Words']) / len(tokens)
                                if len(tokens) != 0 else 0)

    # Count nominalizations
    data['Nominalizations (Total)'] = sum(data['Nominalizations'].values())

    # Count linking adverbs
    data['Linking Adverbs (Total)'] = sum(data['Linking Adverbs'].values())
    data['Additive Adverbs (Total)'] = sum(data['Additive Adverbs'].values())

    # Calculate percentage of tensed verbs that are in present tense
    present_tense = sum(data['Tags'][tag] for tag in ['VBD', 'VBZ'])
    past_tense = data['Tags']['VBD']
    data['Tensed Verb Ratio'] = 0
    if past_tense+present_tense > 0:
        data['Tensed Verb Ratio'] = present_tense / (present_tense+past_tense)

    # Estimate percentage of tensed clauses that are in passive voice
    verb_count = sum(data['Verbs'].values())
    data['Passives/Verb'] = (data['Passives (Total)'] / verb_count)
                             if verb_count != 0 else 0)

    # Calculate percentage of nouns that are immediately preceded by an adjective
    noun_count = sum(data['Nouns'].values())
    data['Modified Noun Ratio'] = (sum(data['Modified Nouns'].values()) / noun_count
                                   if noun_count != 0 else 0)


    return data


def analyze_file(input_filename, output_dir, verbosity=1):
    """Batch processing function for analyzing tagged files.

    Arguments:
        input_filename (str): The input file.
        output_dir (str): The directory of the ouput files.
        verbosity (int): The verbosity of the output.

    Returns:
        (None)

    Raises:
        (None): Will kill the program on IOErrors.
    """
    # Get document text.
    try:
       with open(input_filename, 'r') as f:
           text = f.read()
    except IOError as e:
        print('Error openning file {}: {}'.format(input_filename, e))
        sys.exit(1)

    # Construct output filename.
    global _OUTPUT_EXTENSION
    ext = uweclang.split_ext(os.path.basename(input_filename))[1]
    memo_ext = '.memo' if ext.startswith('.memo') else ''
    output_filename = os.path.join(output_dir,
                                   uweclang.split_ext(os.path.basename(input_filename))[0]
                                   + memo_ext + _OUTPUT_EXTENSION)

    # Open output file.
    try:
        out = io.open(output_filename, 'wb')
    except IOError as e:
        print('Error creating file {}: {}'.format(output_filename, e))
        sys.exit(1)

    # Perform analysis and write to file.
    data = analyze_text(text, input_filename)

    out.write(uweclang.analysis_to_str(data))
    out.close()

    # Print task completion.
    if verbosity >= 1:
        print('Analyzed {} -> "{}"'
              ''.format(os.path.basename(input_filename),
                        os.path.relpath(output_filename)))



if __name__ == '__main__':

    # Create a command line argument parser based on the BATCH_PARSER template.
    # See uweclang/batch/tools.py for details.
    parser = argparse.ArgumentParser(
        description='Analyzes tagged files',
        parents=[uweclang.BATCH_PARSER])


    # Add extra command line arguments for this script.
    parser.add_argument('-x', '--o-extension',
                        nargs='?',
                        default='.res.txt',
                        metavar='oext',
                        dest='oext',
                        help='extension for output file')


    # Parse command line arguments.
    args = parser.parse_args()

    # Set input extensions to get.
    args.extensions = args.extensions or ['.tag.txt']

    # Set default verbosity.
    args.verbose = args.verbose or 1

    # Capture the output extensions into a global variable to that it is
    # visible to extract_plaintext_from_docx.
    _OUTPUT_EXTENSION = args.oext

    # Process the files.
    uweclang.batch_process(analyze_file, **vars(args))


