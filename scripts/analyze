#! /usr/bin/env python

# Python 3 forward compatability imports.
from __future__ import print_function
from __future__ import division
from __future__ import absolute_import
from __future__ import unicode_literals

# Standard imports
import sys
import os
import io
import re
import random
import argparse
import uweclang
from uweclang import is_punctuation, TaggedToken
from itertools import chain
from collections import OrderedDict, Counter


# We want to write all files in UTF-8 encoding. (This is a Python 3 forward
# compatability issue.)
reload(sys)
sys.setdefaultencoding('utf8')

# Script global variable for setting the extension for output files.
_OUTPUT_EXTENSION = ''

def analyze_text(text, file_name=None):
    """Takes a tagged text string and ouputs a dictionary containing the
    analysis of the text.

    Arguments:
        text (str): The input tagged text.
        file_name (Optional[str]): The name of the text file. Defaults to None.

    Returns:
        (dict): The analysis of the text.
    """
    # Prepare random sample of words
    sample = []
    sample_count = 0
    sample_size = 500

    data = OrderedDict([('File', os.path.basename(file_name))])

    tagged = uweclang.read_tagged_string(text)
    tokens = list(chain.from_iterable(tagged))

    # Count sentences and tokens.
    data['Sentences (Total)'] = len(tagged)
    data['Tokens (Total)'] = len(tokens)

    data['Tags'] = Counter()
    data['Words'] = Counter()
    data['Nouns'] = Counter()
    data['Nominalizations'] = Counter()
    data['Verbs'] = Counter()
    data['Modified Nouns'] = Counter()
    data['Additive Adverbs'] = Counter()
    data['Linking Adverbs'] = Counter()
    data['Passives (Total)'] = 0

    # Function to lowercase non-propernoun TaggedTokens
    def lower_nonproper(t):
        if t.tag in ['NNP','NNPS']:
            return t
        return TaggedToken(t.token.lower(), t.tag)


    for t1, t2, t3 in uweclang.nfollowing(tokens, 2):

        # Lowercase non-proper noun words.
        t1, t2, t3 = tuple(lower_nonproper(t) if t is not None else t
                           for t in [t1, t2, t3])

        # Count the number of occurences of each tag and token.
        if not is_punctuation(t1.token):
            data['Words'][t1.token] += 1

            if sample_count < sample_size:
                sample.append(t1)
            else:
                sample[random.SystemRandom().randint(0, sample_size-1)] = t1
            sample_count += 1

        data['Tags'][t1.tag] += 1

        # Count nominalizations
        for suffix in uweclang.NOMINALIZATION_SUFFIXES:
            if t1.token.isalpha() and t1.token.lower().endswith(suffix):
                data['Nominalizations'][suffix] += 1

        # Count verbs
        if t1.tag in ['MD', 'VBD', 'VBP', 'VBZ']:
            data['Verbs'][t1.token] += 1

        # Count nouns
        if t1.tag != None and t1.tag.startswith('NN'):
            data['Nouns'][t1.token] += 1

        if t2 is not None:
            if t3 is not None:
                # Count passives
                if t1.token in uweclang.BE_FORMS and t2[1]=='VBN' or t3[1]=='VBN':
                    data['Passives (Total)'] += 1

            # Count modified nouns
            if (t2.tag is not None and t2.tag.startswith('NN') and
                t1.tag != None and t1.tag.startswith('JJ')):

                data['Modified Nouns'][(t1.token, t2.token)] += 1

            # Count linking adverbs
            if t3 is not None:
                if (t1.token, t2.token, t3.token) in uweclang.LINKING_TRIGRAMS:
                    data['Linking Adverbs'][(t1.token, t2.token, t3.token)] += 1
            elif (t1.token, t2.token) in uweclang.LINKING_BIGRAMS:
                data['Linking Adverbs'][(t1.token, t2.token)] += 1
            elif (t1.token, t2.token) in uweclang.ADDITIVE_BIGRAMS:
                data['Additive Adverbs'][(t1.token, t2.token)] += 1
            elif t1.token in uweclang.LINKING_ADVERBS:
                data['Linking Adverbs'][t1.token] += 1
            elif t1.token in uweclang.ADDITIVE_ADVERBS:
                data['Additive Adverbs'][t1.token] += 1

    # Count words, puntuation, and ratios.
    data['Words (Total)'] = sum(data['Words'].values())
    data['Punc. Tokens (Total)'] = data['Tokens (Total)'] - data['Words (Total)']

    data['Words/Sentence'] = (data['Words (Total)'] / data['Sentences (Total)'] if data['Sentences (Total)'] != 0 else 0)

    data['Tokens/Sentence'] = (data['Tokens (Total)'] / data['Sentences (Total)'] if data['Sentences (Total)'] != 0 else 0)

    data['Tokens/Word'] = (data['Tokens (Total)'] / data['Words (Total)'] if data['Words (Total)'] != 0 else 0)

    # Calculate type/token ratio
    data['Type/Token Ratio'] = uweclang.calculate_type_token_ratio(sample)

    # Count nominalizations
    data['Nominalizations (Total)'] = sum(data['Nominalizations'].values())

    # Count linking adverbs
    data['Linking Adverbs (Total)'] = sum(data['Linking Adverbs'].values())
    data['Additive Adverbs (Total)'] = sum(data['Additive Adverbs'].values())

    # Calculate percentage of tensed verbs that are in present tense
    present_tense = sum(data['Tags'][tag] for tag in ['VBD', 'VBZ'])
    past_tense = data['Tags']['VBD']
    data['Tensed Verb Ratio'] = 0
    if past_tense+present_tense > 0:
        data['Tensed Verb Ratio'] = present_tense / (present_tense+past_tense)

    # Estimate percentage of tensed clauses that are in passive voice
    verb_count = sum(data['Verbs'].values())
    data['Passives/Verb'] = (data['Passives (Total)'] / verb_count if verb_count != 0 else 0)

    # Calculate percentage of nouns that are immediately preceded by an adjective
    noun_count = sum(data['Nouns'].values())
    data['Modified Noun Ratio'] = (sum(data['Modified Nouns'].values()) / noun_count if noun_count != 0 else 0)


    return data


def analyze_file(input_filename, output_dir, verbosity=1):
    """Batch processing function for analyzing tagged files.

    Arguments:
        input_filename (str): The input file.
        output_dir (str): The directory of the ouput files.
        verbosity (int): The verbosity of the output.

    Returns:
        (None)

    Raises:
        (None): Will kill the program on IOErrors.
    """
    # Get document text.
    try:
       with open(input_filename, 'r') as f:
           text = f.read()
    except IOError as e:
        print('Error openning file {}: {}'.format(input_filename, e))
        sys.exit(1)

    # Construct output filename.
    global _OUTPUT_EXTENSION
    ext = uweclang.split_ext(os.path.basename(input_filename))[1]
    memo_ext = '.memo' if ext.startswith('.memo') else ''
    output_filename = os.path.join(output_dir,
                                   uweclang.split_ext(os.path.basename(input_filename))[0]
                                   + memo_ext + _OUTPUT_EXTENSION)

    # Open output file.
    try:
        out = io.open(output_filename, 'wb')
    except IOError as e:
        print('Error creating file {}: {}'.format(output_filename, e))
        sys.exit(1)

    # Perform analysis and write to file.
    data = analyze_text(text, input_filename)

    out.write(uweclang.analysis_to_str(data))
    out.close()

    # Print task completion.
    if verbosity >= 1:
        print('Analyzed {} -> "{}"'
              ''.format(os.path.basename(input_filename),
                        os.path.relpath(output_filename)))



if __name__ == '__main__':

    # Create a command line argument parser based on the BATCH_PARSER template.
    # See uweclang/batch/tools.py for details.
    parser = argparse.ArgumentParser(
        description='Analyzes tagged files',
        parents=[uweclang.BATCH_PARSER])


    # Add extra command line arguments for this script.
    parser.add_argument('-x', '--o-extension',
                        nargs='?',
                        default='.res.txt',
                        metavar='oext',
                        dest='oext',
                        help='extension for output file')


    # Parse command line arguments.
    args = parser.parse_args()

    # Set input extensions to get.
    args.extensions = args.extensions or ['.tag.txt']

    # Set default verbosity.
    args.verbose = args.verbose or 1

    # Capture the output extensions into a global variable to that it is
    # visible to extract_plaintext_from_docx.
    _OUTPUT_EXTENSION = args.oext

    # Process the files.
    uweclang.batch_process(analyze_file, **vars(args))


